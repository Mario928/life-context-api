{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750bd67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q faster-whisper pydub\n",
    "!apt-get -y install ffmpeg > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13eab5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Pick device + compute_type\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    compute_type = \"int8_float16\"  # good mix of speed + quality\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    compute_type = \"int8\"          # CPU will be slow, but works\n",
    "    print(\"Using CPU (this will be slow)\")\n",
    "\n",
    "model = WhisperModel(\n",
    "    \"large-v3\",\n",
    "    device=device,\n",
    "    compute_type=compute_type,\n",
    ")\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ff5a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def make_chunks_with_overlap(\n",
    "    audio_path: str,\n",
    "    chunk_minutes: int = 5,\n",
    "    overlap_seconds: int = 30,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns list of (chunk_audio_segment, chunk_start_time_sec).\n",
    "\n",
    "    - Each chunk is ~chunk_minutes long.\n",
    "    - Each chunk (except the first) starts `overlap_seconds` earlier than the\n",
    "      previous chunk ended.\n",
    "    \"\"\"\n",
    "    audio = AudioSegment.from_file(audio_path)\n",
    "    total_ms = len(audio)\n",
    "\n",
    "    chunk_ms = chunk_minutes * 60 * 1000\n",
    "    overlap_ms = overlap_seconds * 1000\n",
    "\n",
    "    chunks = []\n",
    "    start_ms = 0\n",
    "\n",
    "    while start_ms < total_ms:\n",
    "        end_ms = min(start_ms + chunk_ms, total_ms)\n",
    "        chunk = audio[start_ms:end_ms]\n",
    "        start_sec = start_ms / 1000.0\n",
    "        chunks.append((chunk, start_sec))\n",
    "\n",
    "        if end_ms >= total_ms:\n",
    "            break\n",
    "\n",
    "        # Move start of next chunk: end - overlap\n",
    "        start_ms = end_ms - overlap_ms\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25d514b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def transcribe_with_5min_chunks(\n",
    "    audio_path: str,\n",
    "    chunk_minutes: int = 5,\n",
    "    overlap_seconds: int = 30,\n",
    "    prompt_tail_chars: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    - Splits audio into ~5-min chunks with 30s overlap.\n",
    "    - Uses faster-whisper large-v3 in translate mode (→ English).\n",
    "    - Carries context forward via `initial_prompt` (last N chars of previous text).\n",
    "    - Prints each chunk's transcript as soon as it's processed.\n",
    "    - Returns:\n",
    "        full_text: combined transcript with overlap de-duplicated\n",
    "        all_segments: list of segments with GLOBAL timestamps + language info\n",
    "        language_per_chunk: language metadata per 5-min chunk\n",
    "    \"\"\"\n",
    "    chunks = make_chunks_with_overlap(\n",
    "        audio_path,\n",
    "        chunk_minutes=chunk_minutes,\n",
    "        overlap_seconds=overlap_seconds,\n",
    "    )\n",
    "\n",
    "    print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "    full_text_parts = []\n",
    "    all_segments = []\n",
    "    language_per_chunk = []   # NEW: language info per chunk\n",
    "\n",
    "    prev_tail = \"\"          # for initial_prompt to next chunk\n",
    "    overlap = overlap_seconds\n",
    "\n",
    "    for idx, (chunk_audio, chunk_start_sec) in enumerate(chunks):\n",
    "        print(f\"\\n========== Chunk {idx+1}/{len(chunks)} | starts at {chunk_start_sec:.1f}s ==========\")\n",
    "\n",
    "        # Save chunk to a temporary file for faster-whisper\n",
    "        tmp_path = f\"/tmp/chunk_{idx}.wav\"\n",
    "        chunk_audio.export(tmp_path, format=\"wav\")\n",
    "\n",
    "        # Run transcription on this chunk\n",
    "        segments, info = model.transcribe(\n",
    "            tmp_path,\n",
    "            task=\"translate\",                 # any language -> English\n",
    "            beam_size=5,\n",
    "            vad_filter=True,\n",
    "            condition_on_previous_text=True,  # long-form behavior within chunk\n",
    "            initial_prompt=prev_tail or None,\n",
    "        )\n",
    "\n",
    "        # record language info for this chunk\n",
    "        language_per_chunk.append(\n",
    "            {\n",
    "                \"chunk_index\": idx,\n",
    "                \"start_time_sec\": float(chunk_start_sec),\n",
    "                \"language\": info.language,\n",
    "                \"language_probability\": float(info.language_probability),\n",
    "            }\n",
    "        )\n",
    "        print(\n",
    "            f\"Detected language for this chunk: {info.language} \"\n",
    "            f\"(prob={info.language_probability:.2f})\"\n",
    "        )\n",
    "\n",
    "        # Build text for this chunk (for printing & for global transcript)\n",
    "        chunk_text_for_print = []\n",
    "        chunk_text_for_merge = []\n",
    "\n",
    "        for seg in segments:\n",
    "            local_start = seg.start           # within this chunk\n",
    "            local_end = seg.end\n",
    "            text = seg.text\n",
    "\n",
    "            # For printing: show everything from this chunk\n",
    "            chunk_text_for_print.append(text)\n",
    "\n",
    "            # For the global merged transcript:\n",
    "            #  - convert to global timestamps\n",
    "            #  - skip overlap at START of chunk (to reduce duplicates)\n",
    "            global_start = chunk_start_sec + local_start\n",
    "            global_end = chunk_start_sec + local_end\n",
    "\n",
    "            if idx > 0 and local_start < overlap:\n",
    "                # This is inside the overlapped region at the start of the chunk.\n",
    "                # We *skip* adding it to the global transcript, because it should\n",
    "                # already be covered by the previous chunk.\n",
    "                continue\n",
    "\n",
    "            chunk_text_for_merge.append(text)\n",
    "            all_segments.append(\n",
    "                {\n",
    "                    \"chunk_index\": idx,\n",
    "                    \"start\": float(global_start),\n",
    "                    \"end\": float(global_end),\n",
    "                    \"text\": text,\n",
    "                    # NEW: attach language metadata from this chunk\n",
    "                    \"language\": info.language,\n",
    "                    \"language_probability\": float(info.language_probability),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Join texts\n",
    "        chunk_text_print = \"\".join(chunk_text_for_print)\n",
    "        chunk_text_merge = \"\".join(chunk_text_for_merge)\n",
    "\n",
    "        #  Print this chunk’s transcript immediately\n",
    "        print(chunk_text_print.strip())\n",
    "\n",
    "        # Add to global transcript\n",
    "        full_text_parts.append(chunk_text_merge)\n",
    "\n",
    "        # Update tail prompt for next chunk\n",
    "        if chunk_text_merge:\n",
    "            prev_tail = chunk_text_merge[-prompt_tail_chars:]\n",
    "        elif chunk_text_print:\n",
    "            # fallback: if nothing added (because all in overlap), at least use some print text\n",
    "            prev_tail = chunk_text_print[-prompt_tail_chars:]\n",
    "        else:\n",
    "            prev_tail = prev_tail  # no change if nothing\n",
    "\n",
    "    full_text = \"\".join(full_text_parts)\n",
    "    return full_text, all_segments, language_per_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418fa7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== RUN IT ====\n",
    "audio_file = '/content/drive/MyDrive/R20251107205247.WAV'\n",
    "\n",
    "full_text, segments, language_per_chunk = transcribe_with_5min_chunks(\n",
    "    audio_path=audio_file,\n",
    "    chunk_minutes=5,\n",
    "    overlap_seconds=30,\n",
    "    prompt_tail_chars=300,   # you can tweak this\n",
    ")\n",
    "\n",
    "# Save combined transcript + segments with timestamps + language metadata\n",
    "Path(\"transcript_en_merged.txt\").write_text(full_text, encoding=\"utf-8\")\n",
    "Path(\"transcript_en_segments.json\").write_text(\n",
    "    json.dumps(segments, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "Path(\"language_per_chunk.json\").write_text(\n",
    "    json.dumps(language_per_chunk, ensure_ascii=False, indent=2),\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "print(\"\\nSaved transcript_en_merged.txt, transcript_en_segments.json, language_per_chunk.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
